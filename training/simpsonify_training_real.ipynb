{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simpsonify LoRA Training (Kohya_ss)\n",
        "\n",
        "**Professional cartoon-style LoRA training using Kohya's training scripts**\n",
        "\n",
        "This notebook trains a LoRA model to transform photos into cartoon-style images.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuration Summary\n",
        "\n",
        "- **Base Model**: Stable Diffusion v1.5\n",
        "- **Training Method**: Kohya sd-scripts with LoRA\n",
        "- **LoRA Rank**: 16, Alpha: 8-16\n",
        "- **Resolution**: 512x512\n",
        "- **Epochs**: 10-12\n",
        "- **Expected Duration**: ~60-100 minutes on T4 GPU\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/upgrade PyTorch (optional - if needed)\n",
        "# Uncomment if you need a specific PyTorch version\n",
        "\n",
        "# !pip -q uninstall -y torch torchvision torchaudio\n",
        "# !pip -q install torch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "upgrade_torch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone Kohya Training Scripts"
      ],
      "metadata": {
        "id": "clone"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone Kohya sd-scripts repository\n",
        "!cd /content\n",
        "!rm -rf kohya_ss\n",
        "!git clone --recurse-submodules https://github.com/bmaltais/kohya_ss.git\n",
        "\n",
        "print(\"‚úì Kohya scripts cloned\")"
      ],
      "metadata": {
        "id": "clone_kohya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip install -U pip\n",
        "!pip install -q accelerate diffusers transformers safetensors einops tqdm pillow voluptuous\n",
        "\n",
        "# Check accelerate version\n",
        "import accelerate\n",
        "print(\"accelerate:\", accelerate.__version__)"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Upload and Prepare Dataset"
      ],
      "metadata": {
        "id": "dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your dataset ZIP file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"‚úì File uploaded\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Find the uploaded zip file\n",
        "zip_file = [f for f in uploaded.keys() if f.endswith('.zip')][0]\n",
        "\n",
        "# Extract to /content/dataset/train/\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset/train')\n",
        "\n",
        "print(\"‚úì Dataset extracted to /content/dataset/train\")\n",
        "!ls -la /content/dataset/train"
      ],
      "metadata": {
        "id": "extract_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate captions for all images\n",
        "from pathlib import Path\n",
        "\n",
        "# Adjust this path to match your extracted dataset structure\n",
        "# If images are in a subfolder, update this:\n",
        "img_dir = Path(\"/content/dataset/train/20_simpsons\")  # Adjust if needed\n",
        "\n",
        "# If images are directly in /content/dataset/train:\n",
        "# img_dir = Path(\"/content/dataset/train\")\n",
        "\n",
        "imgs = sorted(p for p in img_dir.iterdir() if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"])\n",
        "\n",
        "# Caption text - customize this based on your style!\n",
        "# For Simpsons style:\n",
        "caption = \"simpsons_style, cartoon, animated\"\n",
        "\n",
        "# For general cartoon style:\n",
        "# caption = \"cartoonify, 2D cartoon, flat colors, simple shapes\"\n",
        "\n",
        "# Create .txt file for each image\n",
        "for p in imgs:\n",
        "    p.with_suffix(\".txt\").write_text(caption, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"‚úì Created captions for {len(imgs)} images\")\n",
        "print(f\"Sample caption: {imgs[0].with_suffix('.txt').read_text()}\")"
      ],
      "metadata": {
        "id": "create_captions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configure Training Parameters\n",
        "\n",
        "**Two training configurations are provided:**\n",
        "- **Config A**: Simpsons Style (original settings)\n",
        "- **Config B**: Cartoonify Style (optimized settings)\n",
        "\n",
        "Choose one by running the corresponding cell below."
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIG A: Simpsons Style (Basic) ===\n",
        "\n",
        "CONFIG = {\n",
        "    \"output_name\": \"simpsons_style_lora\",\n",
        "    \"network_dim\": 16,\n",
        "    \"network_alpha\": 16,\n",
        "    \"max_train_epochs\": 12,\n",
        "    \"learning_rate\": \"1e-4\",\n",
        "    \"unet_lr\": \"1e-4\",\n",
        "    \"text_encoder_lr\": \"5e-5\",\n",
        "    \"lr_scheduler\": \"constant\",\n",
        "    \"optimizer_type\": \"AdamW\",\n",
        "    \"save_every_n_epochs\": 1,\n",
        "    \"additional_args\": \"\"\n",
        "}\n",
        "\n",
        "print(\"‚úì Config A (Simpsons Style) loaded\")\n",
        "print(f\"Training for {CONFIG['max_train_epochs']} epochs\")\n",
        "print(f\"LoRA rank: {CONFIG['network_dim']}, alpha: {CONFIG['network_alpha']}\")"
      ],
      "metadata": {
        "id": "config_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIG B: Cartoonify Style (Advanced) ===\n",
        "\n",
        "CONFIG = {\n",
        "    \"output_name\": \"cartoonify_lora\",\n",
        "    \"network_dim\": 16,\n",
        "    \"network_alpha\": 8,\n",
        "    \"max_train_epochs\": 10,\n",
        "    \"learning_rate\": \"5e-5\",\n",
        "    \"unet_lr\": \"5e-5\",\n",
        "    \"text_encoder_lr\": \"5e-5\",\n",
        "    \"lr_scheduler\": \"cosine_with_restarts\",\n",
        "    \"lr_scheduler_num_cycles\": 3,\n",
        "    \"lr_warmup_steps\": 50,\n",
        "    \"optimizer_type\": \"AdamW8bit\",\n",
        "    \"save_every_n_epochs\": 2,\n",
        "    \"additional_args\": \"\"\"\n",
        "  --xformers \\\n",
        "  --cache_latents \\\n",
        "  --cache_latents_to_disk \\\n",
        "  --noise_offset=0.05 \\\n",
        "  --adaptive_noise_scale=0.00357 \\\n",
        "  --min_snr_gamma=5 \\\n",
        "  --max_grad_norm=1.0\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "print(\"‚úì Config B (Cartoonify - Advanced) loaded\")\n",
        "print(f\"Training for {CONFIG['max_train_epochs']} epochs\")\n",
        "print(f\"LoRA rank: {CONFIG['network_dim']}, alpha: {CONFIG['network_alpha']}\")\n",
        "print(\"Advanced features: xformers, noise offset, min-SNR gamma\")"
      ],
      "metadata": {
        "id": "config_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start Training\n",
        "\n",
        "‚ö†Ô∏è **This will take 60-100 minutes depending on your dataset size and epochs**"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directories\n",
        "!mkdir -p /content/output /content/logs\n",
        "\n",
        "# Build training command\n",
        "base_cmd = f\"\"\"\n",
        "cd /content/kohya_ss/sd-scripts && python train_network.py \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        "  --train_data_dir=\"/content/dataset/train\" \\\n",
        "  --output_dir=\"/content/output\" \\\n",
        "  --output_name=\"{CONFIG['output_name']}\" \\\n",
        "  --save_model_as=\"safetensors\" \\\n",
        "  --caption_extension=\".txt\" \\\n",
        "  --network_module=\"networks.lora\" \\\n",
        "  --network_dim={CONFIG['network_dim']} \\\n",
        "  --network_alpha={CONFIG['network_alpha']} \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --max_train_epochs={CONFIG['max_train_epochs']} \\\n",
        "  --learning_rate={CONFIG['learning_rate']} \\\n",
        "  --unet_lr={CONFIG['unet_lr']} \\\n",
        "  --text_encoder_lr={CONFIG['text_encoder_lr']} \\\n",
        "  --lr_scheduler=\"{CONFIG['lr_scheduler']}\" \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --save_precision=\"fp16\" \\\n",
        "  --optimizer_type=\"{CONFIG['optimizer_type']}\" \\\n",
        "  --gradient_checkpointing \\\n",
        "  --max_data_loader_n_workers=2 \\\n",
        "  --save_every_n_epochs={CONFIG['save_every_n_epochs']} \\\n",
        "  --seed=42 \\\n",
        "  --console_log_level=\"INFO\" \\\n",
        "  --console_log_file=\"/content/logs/train.log\"\n",
        "\"\"\"\n",
        "\n",
        "# Add scheduler-specific params if using cosine_with_restarts\n",
        "if CONFIG['lr_scheduler'] == 'cosine_with_restarts':\n",
        "    base_cmd += f\" --lr_scheduler_num_cycles={CONFIG.get('lr_scheduler_num_cycles', 3)}\"\n",
        "    base_cmd += f\" --lr_warmup_steps={CONFIG.get('lr_warmup_steps', 50)}\"\n",
        "\n",
        "# Add additional args\n",
        "base_cmd += CONFIG.get('additional_args', '')\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "print(\"Command:\")\n",
        "print(base_cmd)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run training\n",
        "!{base_cmd}"
      ],
      "metadata": {
        "id": "run_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Check Training Log"
      ],
      "metadata": {
        "id": "check_log"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View last 50 lines of training log\n",
        "!tail -50 /content/logs/train.log"
      ],
      "metadata": {
        "id": "view_log"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all generated checkpoints\n",
        "!ls -lh /content/output/*.safetensors"
      ],
      "metadata": {
        "id": "list_checkpoints"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Trained Models\n",
        "\n",
        "Generate test images with different checkpoints to compare quality."
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "\n",
        "# Load base pipeline\n",
        "BASE = \"runwayml/stable-diffusion-v1-5\"\n",
        "OUTDIR = Path(\"/content/test_outputs\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    BASE,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None,\n",
        ").to(\"cuda\")\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "print(\"‚úì Pipeline loaded\")"
      ],
      "metadata": {
        "id": "load_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test(lora_path: str, tag: str, prompts, seed=42, steps=25, cfg=7.0):\n",
        "    \"\"\"Test a LoRA checkpoint with multiple prompts\"\"\"\n",
        "    \n",
        "    # Load LoRA\n",
        "    pipe.load_lora_weights(lora_path)\n",
        "    \n",
        "    # Optional: adjust LoRA strength\n",
        "    try:\n",
        "        pipe.set_adapters([\"default\"], adapter_weights=[1.0])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    g = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "\n",
        "    images = []\n",
        "    for i, p in enumerate(prompts, 1):\n",
        "        img = pipe(\n",
        "            prompt=p,\n",
        "            negative_prompt=\"blurry, deformed, extra fingers, watermark, text\",\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            generator=g,\n",
        "            height=512,\n",
        "            width=512,\n",
        "        ).images[0]\n",
        "        \n",
        "        fp = OUTDIR / f\"{tag}_p{i}.png\"\n",
        "        img.save(fp)\n",
        "        images.append(img)\n",
        "        display(img)\n",
        "\n",
        "    # Unload LoRA\n",
        "    try:\n",
        "        pipe.unload_lora_weights()\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    return images"
      ],
      "metadata": {
        "id": "test_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompts - customize based on your trigger word!\n",
        "\n",
        "# For simpsons_style:\n",
        "prompts = [\n",
        "    \"portrait photo of a person, simpsons_style, cartoon, animated\",\n",
        "    \"full body, outdoor street scene, simpsons_style, cartoon, animated\",\n",
        "    \"group of people, living room, simpsons_style, cartoon, animated\",\n",
        "]\n",
        "\n",
        "# For cartoonify:\n",
        "# prompts = [\n",
        "#     \"portrait of a woman, cartoonify, 2D cartoon, flat colors\",\n",
        "#     \"portrait of a man with beard, cartoonify, simple shapes, bold outline\",\n",
        "#     \"smiling person, cartoonify, animated style\",\n",
        "# ]"
      ],
      "metadata": {
        "id": "test_prompts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test multiple checkpoints\n",
        "# Adjust epoch numbers based on your CONFIG['save_every_n_epochs']\n",
        "\n",
        "output_name = CONFIG['output_name']\n",
        "\n",
        "lora_files = {\n",
        "    \"e06\": f\"/content/output/{output_name}-000006.safetensors\",\n",
        "    \"e08\": f\"/content/output/{output_name}-000008.safetensors\",\n",
        "    \"e10\": f\"/content/output/{output_name}-000010.safetensors\",\n",
        "    \"final\": f\"/content/output/{output_name}.safetensors\",\n",
        "}\n",
        "\n",
        "# Test each checkpoint\n",
        "for tag, path in lora_files.items():\n",
        "    if Path(path).exists():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Testing: {tag}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        run_test(path, tag, prompts)\n",
        "    else:\n",
        "        print(f\"‚ö† Checkpoint not found: {path}\")\n",
        "\n",
        "print(\"\\n‚úì All tests complete!\")\n",
        "print(f\"Images saved to: {OUTDIR}\")"
      ],
      "metadata": {
        "id": "run_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare Checkpoints Side-by-Side"
      ],
      "metadata": {
        "id": "compare"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Quick comparison between two checkpoints\n",
        "prompt = \"portrait of a woman, cartoonify, 2D cartoon, flat colors\"\n",
        "epochs_to_compare = [6, 8]  # Adjust based on available checkpoints\n",
        "\n",
        "images = []\n",
        "for epoch in epochs_to_compare:\n",
        "    lora_path = f\"/content/output/{output_name}-{epoch:06d}.safetensors\"\n",
        "    \n",
        "    if not Path(lora_path).exists():\n",
        "        print(f\"‚ö† Checkpoint not found: {lora_path}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"Testing epoch {epoch}...\")\n",
        "    \n",
        "    pipe.load_lora_weights(lora_path)\n",
        "    pipe.fuse_lora()\n",
        "    \n",
        "    img = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
        "    images.append(img)\n",
        "    \n",
        "    pipe.unfuse_lora()\n",
        "\n",
        "# Create comparison grid\n",
        "if images:\n",
        "    grid_width = 512 * len(images)\n",
        "    grid = Image.new('RGB', (grid_width, 512))\n",
        "    for i, img in enumerate(images):\n",
        "        grid.paste(img, (512*i, 0))\n",
        "\n",
        "    grid.save(\"/content/comparison.png\")\n",
        "    display(grid)\n",
        "    print(\"‚úì Comparison saved: comparison.png\")\n",
        "else:\n",
        "    print(\"No images generated for comparison\")"
      ],
      "metadata": {
        "id": "compare_checkpoints"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Download Trained Models"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# List all available models\n",
        "print(\"Available models:\\n\")\n",
        "!ls -lh /content/output/*.safetensors\n",
        "\n",
        "# Download the final model\n",
        "final_model = f\"/content/output/{CONFIG['output_name']}.safetensors\"\n",
        "if os.path.exists(final_model):\n",
        "    print(f\"\\nDownloading: {final_model}\")\n",
        "    files.download(final_model)\n",
        "    print(\"‚úì Download started!\")\n",
        "else:\n",
        "    print(f\"‚ö† Model not found: {final_model}\")"
      ],
      "metadata": {
        "id": "download_final"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a specific checkpoint\n",
        "checkpoint_epoch = 8  # Change this to download different epochs\n",
        "\n",
        "checkpoint_file = f\"/content/output/{CONFIG['output_name']}-{checkpoint_epoch:06d}.safetensors\"\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(f\"Downloading checkpoint from epoch {checkpoint_epoch}...\")\n",
        "    files.download(checkpoint_file)\n",
        "    print(\"‚úì Download started!\")\n",
        "else:\n",
        "    print(f\"‚ö† Checkpoint not found: {checkpoint_file}\")"
      ],
      "metadata": {
        "id": "download_checkpoint"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Download** your `.safetensors` file(s)\n",
        "2. **Choose best checkpoint** based on test images\n",
        "3. **Rename** (e.g., `my_cartoon_style.safetensors`)\n",
        "4. **Copy** to your project:\n",
        "   ```bash\n",
        "   cp my_cartoon_style.safetensors /path/to/simpsonify/backend/models/\n",
        "   ```\n",
        "5. **Update** `backend/.env`:\n",
        "   ```\n",
        "   SD_LORA_PATH=/path/to/backend/models/my_cartoon_style.safetensors\n",
        "   ```\n",
        "6. **Restart** backend and test!\n",
        "\n",
        "---\n",
        "\n",
        "## Training Tips\n",
        "\n",
        "### Loss Analysis\n",
        "- **Good training**: Loss decreases steadily (0.15 ‚Üí 0.04 ‚Üí 0.02)\n",
        "- **Overfitting**: Loss plateaus early and doesn't improve\n",
        "- **Optimal checkpoint**: Usually 60-80% through training\n",
        "\n",
        "### Configuration Guide\n",
        "\n",
        "**For stronger style transfer:**\n",
        "- Increase `network_alpha` (16 ‚Üí 32)\n",
        "- More epochs (12 ‚Üí 15)\n",
        "- Lower learning rate (1e-4 ‚Üí 5e-5)\n",
        "\n",
        "**For subtle effects:**\n",
        "- Lower `network_alpha` (16 ‚Üí 8)\n",
        "- Fewer epochs (12 ‚Üí 8)\n",
        "- Higher learning rate\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Training! üé®**"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}
